{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class CarEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Simple car speed control environment with improved physics and SAC-friendly rewards.\n",
    "    Action: acceleration in [-5, 5] m/sÂ²\n",
    "    Observation: [current_speed, error_to_setpoint]\n",
    "    Reward: -abs(error_to_setpoint) + bonus when close.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CarEnv, self).__init__()\n",
    "        self.setpoint = 60.0  # target speed (km/h)\n",
    "        self.dt = 0.1         # timestep (s)\n",
    "        self.max_steps = 500\n",
    "        \n",
    "        self.action_space = spaces.Box(low=-5.0, high=5.0, shape=(1,), dtype=np.float32)\n",
    "        obs_low = np.array([0.0, -100.0], dtype=np.float32)\n",
    "        obs_high = np.array([200.0, 100.0], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def step(self, action):\n",
    "        accel = np.clip(float(action[0]), -5.0, 5.0)\n",
    "        drag_coeff = 0.01\n",
    "        rolling_resistance = 0.005\n",
    "        drag_force = drag_coeff * self.v ** 2\n",
    "        rolling_force = rolling_resistance * self.v\n",
    "        self.v += (accel - drag_force - rolling_force) * self.dt\n",
    "        self.v = max(0.0, self.v)\n",
    "        error = self.setpoint - self.v\n",
    "        \n",
    "        reward = -abs(error)\n",
    "        if abs(error) < 2.0:\n",
    "            reward += 10.0  # Strong incentive to stay near setpoint\n",
    "        \n",
    "        self.step_count += 1\n",
    "        terminated = self.step_count >= self.max_steps\n",
    "        truncated = False\n",
    "        \n",
    "        obs = np.array([self.v, error], dtype=np.float32)\n",
    "        info = {}\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.v = np.random.uniform(0, 10)\n",
    "        self.step_count = 0\n",
    "        error = self.setpoint - self.v\n",
    "        obs = np.array([self.v, error], dtype=np.float32)\n",
    "        info = {}\n",
    "        return obs, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "# Vectorize and normalize environment\n",
    "env = make_vec_env(CarEnv, n_envs=1)\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# Initialize SAC model\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./sac_car_tb/\")\n",
    "\n",
    "# Train\n",
    "model.learn(total_timesteps=200_000)\n",
    "\n",
    "# Save model and normalization stats\n",
    "model.save(\"sac_car\")\n",
    "env.save(\"sac_car_env.pkl\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b83c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
